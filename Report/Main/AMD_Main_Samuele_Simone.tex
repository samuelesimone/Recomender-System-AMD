% !TeX spellcheck = en_US 
\documentclass[12pt,english]{report}
\usepackage{tesi}
% CORSO DI LAUREA:
\def\myCDL{Master in\\Computer Science}

% TITOLO REPORT:
\def\myTitle{Algorithms for massive datasets \\
\large{Final report about Recommender System}}

% AUTORE:
\def\myName{Samuele Simone}
\def\myMat{Matr. Nr. 11910A}

\def\myRefereeA{Prof. Dario Malchiodi}

% ANNO ACCADEMICO
\def\myYY{2022-2023}

% Il seguente comando introduce un elenco delle figure dopo l'indice (facoltativo)
%\figurespagetrue

% Il seguente comando introduce un elenco delle tabelle dopo l'indice (facoltativo)
%\tablespagetrue


% Package di formato
\usepackage[a4paper]{geometry}		% Formato del foglio
\usepackage[english]{babel}			% Supporto per l'italiano
\usepackage[utf8]{inputenc}			% Supporto per UTF-8
\usepackage[a-1b]{pdfx}			% File conforme allo standard PDF-A (obbligatorio per la consegna)

% Package per la grafica
\usepackage{graphicx}				% Funzioni avanzate per le immagini
\usepackage{hologo}					% Bibtex logo with \hologo{BibTeX}
%\usepackage{epsfig}				% Permette immagini in EPS
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}

%Creating dark code theme for listings
\definecolor{codegreen}{rgb}{0.58,0.88,0.58}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codeorange}{rgb}{0.72,0.54,0.45}
\definecolor{backcolour}{rgb}{0.10,0.13,0.14}
\definecolor{myorange}{RGB}{245,156,74}
\definecolor{keyw}{rgb}{0.60,0.85,0.98}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{keyw},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily\footnotesize \color{white},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% Package tipografici
\usepackage{amssymb,amsmath,amsthm} % Simboli matematici
\usepackage{listings}				% Scrittura di codice

% Package ipertesto
\usepackage{url}					% Visualizza e rendere interattii gli URL
\usepackage{hyperref}				% Rende interattivi i collegamenti interni
\usepackage{notes2bib}

\usepackage{multirow}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=myorange,
    filecolor=magenta,  
    }
\begin{document}
% Creazione automatica del frontespizio
\frontespizio
{\raggedleft \large \sl \textit{I/We declare that this material, which I/We now submit for assessment, is entirely my/our own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my/our work. I/We understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by me/us or any other person for assessment on this or any other course of study.} \\}


\afterpreface

The report is made up of 7 chapters based on this template\cite{latextemp} and adapted for our purpose. In the \textbf{Chapter~\ref{ch:introduction}} we give at the reader a general view of what is a recommender system and where we can find it.
\textbf{Chapter~\ref{ch:environment}}, we start to discuss about the development setup for the project. Then, in the \textbf{Chapter~\ref{ch:dataset}} we focus on the dataset, how is composed and we explore the data. Later, in the \textbf{Chapter~\ref{ch:recsys}} we explain the recommender system. the different approaches and the mechanisms behind. Then in the \textbf{Chapter \ref{ch:recsyscontbased}} we enter deeper by showing the code of different approaches and some experiments. Some notion about scalability and complexity are given in the \textbf{Chapter~\ref{ch:scalability}}. In order to evaluate the project developed we are talking about some metrics like CG,NDCG. Consequently , we summarize the aspects and the results obtained during the various  in the \textbf{Chapter~\ref{ch:resultsconc}}.

\chapter{Introduction}\label{ch:introduction}
A recommender system is used everywhere nowadays. Indeed all big companies are pushing in these systems because they can increase the sells about their product, e.g., when we are scrolling a product on Amazon, then they show us a list of recommendation based on the item selected.\par

Recommendation system use a number of different technologies. We can split these system into two broad groups: \cite{rajaraman2014mining}
\begin{itemize}
\item \textit{Content-based systems}: examine properties of the items recommended. For instance, if a Netflix user has watched many cowboy movies, then recommend a movie classified in the database as having the "cowboy" genre.
\item \textit{Collaborative filtering} systems recommend items based on similarity measures between users and/or items.
\end{itemize}

\chapter{Environment setup}\label{ch:environment}
Before going deeper into the project we must discuss about the entire environment was setup. Indeed I used different libraries in order to create the recommender system.
Here the snippet of all libraries used in the project:
\begin{lstlisting}[language={Python},label={lst:libraries},caption={Loading all the python libraries}]
#importing the required pyspark library
import pyspark
from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.recommendation import ALS
from pyspark.sql.functions import split
from pyspark.sql.functions import array_contains
from pyspark.sql.functions import col
from pyspark.sql.functions import from_json
from pyspark.sql.functions import when
from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.feature import Normalizer
from pyspark.ml.linalg import Vectors
from pyspark.sql import Row
from pyspark.sql.window import Window
#import for manage global var
import os
#import for graphics
import matplotlib.pyplot as plt
import pandas as pd
#import for regular expression
import re
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
\end{lstlisting}
\section{Pyspark setup}
PySpark is the Python API for Apache Spark. It enables you to perform real-time, large-scale data processing in a distributed environment using Python. It also provides a PySpark shell for interactively analyzing your data. \par
PySpark combines Python’s learnability and ease of use with the power of Apache Spark to enable processing and analysis of data at any size for everyone familiar with Python.\par
PySpark supports all of Spark’s features such as Spark SQL, DataFrames, Structured Streaming, Machine Learning (MLlib) and Spark Core. \cite{pysparkoverview} \par
Refering to the Listing \ref{lst:libraries} we can see that there are several libraries about PySpark. I will discuss about the most important:
\begin{itemize}
\item \textbf{SparkConf}: Configuration for a Spark application. Used to set various Spark parameters as key-value pairs. \cite{sparkconf}
\item \textbf{SparkSession}:The entry point to programming Spark with the Dataset and DataFrame API. \cite{sparksession}
\item \textbf{pyspark.ml}: DataFrame-based machine learning APIs to let users quickly assemble and configure practical machine learning pipelines. \cite{pysparkml}
\item \textbf{pyspark.sql.functions}:A list of function that allow the user to explore dataframe \cite{pysparkfunc} 
\end{itemize}
In the project I used PySpark Dataframe. They are distributed collections of data that can be run on multiple machines and organize data into named columns. These dataframes can pull from external databases, structured data files or existing resilient distributed datasets (RDDs).
\section{Kaggle setup}
The data were hosted on the Kaggle platform. Kaggle is a subsidiary of Google, it is an online community of data scientists and machine learning engineers.\par
Kaggle allows users to find datasets they want to use in building AI models, publish datasets, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\par
Kaggle got its start in 2010 by offering machine learning and data science competitions as well as offering a public data and cloud-based business platform for data science and AI education. \cite{kaggle}\par
In order to access to the data Kaggle platform gives to every register account an API credential that is necessary to download the datasets.
\begin{lstlisting}[language={Python},label={lst:kagglecredential},caption={API Credential for accessing the data}]
os.environ['KAGGLE_USERNAME'] = 'your_kaggle_username'
os.environ['KAGGLE_KEY'] = 'your_kaggle_key'
\end{lstlisting}
\section{Pandas setup}
Pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. It is already well on its way toward this goal. \cite{pandas}\par
I used it to explore the data, as we will see in the chapter \ref{ch:dataset}, and to build the content-based recommendation system. My goal was to implement the recommendation system with different types of approaches, and Pandas is one of them. However, in the course of the paper I will emphasize the different ways in which I tried my hand at it.
\section{Scikit-learn setup}
Scikit-learn is a library in Python that provides many unsupervised and supervised learning algorithms. It’s built upon some of the technology like NumPy, pandas, and Matplotlib. 
The functionality that scikit-learn provides include \cite{scikit}:
\begin{itemize}
\item Regression, including Linear and Logistic Regression
\item Classification, including K-Nearest Neighbors
\item Clustering, including K-Means and K-Means++
\item Model selection
\item Preprocessing, including Min-Max Normalization
\end{itemize}
Indeed some of these functionality are used in this project such as the K-NN, the second different approach to build the Content based recommender system.
\section{Numpy setup}
NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. \cite{numpy}\par
It was useful in constructing the function \texttt{cosine\textunderscore similarity\textunderscore scratch} to perform multiplication and normalization of vectors.
\chapter{Dataset: A look inside}\label{ch:dataset}
\section{Data loading}
We start to look how the data is loaded inside the project.
First of all the data is downloaded from Kaggle with this code:
\begin{lstlisting}[language={bash},label={lst:datadownload},caption={Download dateset}]
!kaggle datasets download -d yelp-dataset/yelp-dataset
\end{lstlisting}
and proceeded to unzip the dataset obtaining:
\begin{lstlisting}[language={bash},label={lst:datajson},caption={Unzip dateset}]
Archive:  /content/yelp-dataset.zip
  inflating: yelp-dataset/Dataset_User_Agreement.pdf  
  inflating: yelp-dataset/yelp_academic_dataset_business.json  
  inflating: yelp-dataset/yelp_academic_dataset_checkin.json  
  inflating: yelp-dataset/yelp_academic_dataset_review.json  
  inflating: yelp-dataset/yelp_academic_dataset_tip.json  
  inflating: yelp-dataset/yelp_academic_dataset_user.json 
\end{lstlisting}
These json files are huge and to manage it we load the data into a Pyspark df with this command:
\begin{lstlisting}[language={Python},label={lst:loadjson},caption={Loading json data into Pyspark df}]
df_review = spark.read.json('/content/yelp-dataset/yelp_academic_dataset_review.json')
\end{lstlisting}
We repeat this operation many time as the number of the json files.
So we obtain these dataframe:
\begin{itemize}
\item \textbf{df\textunderscore review}: (review\textunderscore id,user\textunderscore id,business\textunderscore id,stars\textunderscore review). These are the attributes I selected from the df. However, there is one field that I think should be mentioned which is the text field, very useful for doing further analysis. In my case it was not selected as I believe that the quoted data is sufficient for our purposes.
\item \textbf{df\textunderscore users}:(user\textunderscore id,username,average\textunderscore stars)
\item \textbf{df\textunderscore business}: (business\textunderscore id,address,attributes,categories,city,is\textunderscore open,name,stars,state). There are others column that I prefer to remove because is not adding values for our analysis.
\end{itemize}
\section{Visualizing the data}\label{sec:visdata}
To give a main idea of the data I create a chart where on the x-axes there are all the states and on the y-axes the count of the total business for that particular state. 
\begin{figure}[hbtp]
\caption{Business counting per state}
\label{fig:buscount}
\centering
\includegraphics[scale=0.5]{../Images/dataexp.png}
\end{figure}
From the Figure \ref{fig:buscount} we can see that in "PA", we refer to Pennsylvania, a state located in the northeastern part of the United States. So I decided to conduct my analysis over the "PA" state due the amount of businesses present over there. Furthermore due the high dimension of the df it's difficult to create others type of graphics.
\section{Data Filtering}
As we discussed in the previous section \ref{sec:visdata} we want to targeting our recommender system in a specific area for a specific type of business. So I decided to take \textit{"PA"} as state and \textit{"Restaurant"} as type of business. Moreover I filtered the new dataframe called \texttt{restaurant\textunderscore df} with another parameter called \texttt{is\textunderscore open} that when is equal to 1 means that this business is actually a running business.
\begin{lstlisting}[language={Python},label={lst:filtdata},caption={Filtering Pyspark df}]
# Filtering data
restaurant_df = df_business_pandas[(df_business_pandas['state'] == state) & (df_business_pandas['is_open'] == 1) & df_business_pandas['categories'].str.contains(type_business)==True].reset_index()
\end{lstlisting}
Here an example of \texttt{restaurant\textunderscore df} printed:
\begin{table}[h]
\centering
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{llllllllll}
\hline
\multirow{2}{*}{index} & \multirow{2}{*}{address} & \multirow{2}{*}{attributes} & \multirow{2}{*}{business\_id} & \multirow{2}{*}{categories} & \multirow{2}{*}{city} & \multirow{2}{*}{is\_open} & \multirow{2}{*}{name} & \multirow{2}{*}{stars} & \multirow{2}{*}{state} \\
 & & & & & & & & & \\
\hline
3 & 935 Race St & (None, None, 'full\_bar', \{'touristy': False, '... & MTSW4McQd7CbVtyjqoe9mw & Restaurants, Food, Bubble Tea, Coffee \& Tea, B... & Philadelphia & 1 & St Honore Pastries & 4.0 & "PA" \\
\hline
\end{tabular}%
}
\caption{Example of a restaurant\_df's row}
\label{tab:rowrest}
\end{table}
Note that the attribute and category columns must be preprocessed before being used.
\section{Data preprocessing}
Lets start to analyze the attribute column. As we can see they are in this form:
\begin{lstlisting}[language={Python},label={lst:attritemex},caption={Attributes item example}]
Row(AcceptsInsurance=None, AgesAllowed=None, Alcohol="u'none'", Ambience=None, BYOB=None,..)
\end{lstlisting}
So we need a function that allow us to extract key-value pairs in a format understandable to Python. Here the snippet:
\begin{lstlisting}[language={Python},escapechar=|,label={lst:extractvalue},caption={Extracting key-value from attributes}]
# Function for extracting key-value from attributes
def extract_values(row):
    if row is None or row == "{}":
        return {}
    
    attributes = {}
    pattern = r"(\w+)\s*=\s*([^,]+)" |\label{line:re}|
    matches = re.findall(pattern, row)
    
    for key, value in matches:
        value = value.strip("'")
        attributes[key] = value
    
    return attributes
\end{lstlisting}
To achieve the result I used the regular expression. More specifically the line~\ref{line:re} defines a regular expression pattern.The pattern captures two groups:
\begin{itemize}
 \item the key (composed of one or more word characters \texttt{\textbackslash w$+$})
 \item the value (composed of one or more characters that are not a comma \texttt{\textbackslash $[\hat,]+$}), separated by an equal sign with optional whitespace around it.
 \end{itemize} 
Then we apply the function over all the rows of column attributes.
Now that the format seems to be good we proceed to create an entire \texttt{restaurant\textunderscore df\textunderscore attr} in which the attributes are extracted making a new column of the df and populating it with 0 or 1 based on their original value. An example are in the Table \ref{tab:restattrtable}.
\\
\begin{table}[]
\caption{Example of \texttt{restaurant\textunderscore df\textunderscore attr} elements} 
\label{tab:restattrtable}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
BestNights & RestaurantsCounterService & WiFi & ... & Smoking & CoatCheck \\ \hline
0          & 1                         & 0    & ... & 0       & 0         \\ \hline
1          & 0                         & 1    & ... & 1       & 1         \\ \hline
\end{tabular}
\end{table}
We apply this reasoning also for categories obtaining the \texttt{df\textunderscore categories\textunderscore dm}. It will be useful to create the \texttt{df\textunderscore total}, a fundamental component for the recommender system. So in the Chapter \ref{ch:recsys} we are going to understand how recommendation systems work in theory.
\chapter{Recommender system}\label{ch:recsys}
As we mentioned in the Chapter \ref{ch:introduction}, there are two main basic architectures for a recommendation system. In my project I developed from scratch  more on a \textit{Content-Based} recommendation system. They are based on properties of items and the similarity of these are determined by measuring the similarity in their properties.
However I also worked with the ALS as \textit{Collaborative Filtering} using the PySpark library to compare mine with the one developed with the library. In this Chapter we give to the reader the general notion behind recommender system based on the reference book \cite{rajaraman2014mining}.
\section{Content-Based Recommendations}
We are starting to understand what is an item profiles. It's a record or collection of records representing important characteristics of the item. For example if we consider the restaurants as type of business of this project, we can say that the features of a restaurants can be all the attributes such as \textit{GoodForKids,BusinessAcceptsBitcoin,DogsAllowed...}.
In order to be a recommender system we need also the user. Indeed also the users has their profiles that describes their preferences. \par
Another fundamental aspect is the utility matrix. It's giving for each user-item pair, a value that represents what is known about the degree of preference of that user for that item. Without a utility matrix, it is almost impossible to recommend items. Usually there are two main approaches: 
\begin{itemize}
 \item We can ask users to evaluate items. We fall in this case because the from the dataset \texttt{review} we can access to all user-business pair reviews with the respective rate.
 \item We can make inferences from user's behavior. Indeed if one user buy something online or just watched a video we can say that he liked it.
 \end{itemize} 
Our ultimate goal for content-based recommendation is to create both a item profile consisting of feature-value pairs and a user profile summarizing the preferences of the user,based of their row of the utility matrix.If we consider profiles as vectors we can compute the cosine similarity in the space of the features and understand if the vectors appears closer or not. In this way we can recommend business to the user based on his preferences.
\section{Collaborative Filtering}
When we are referring to Collaborative Filtering we are focusing on the similarity of the user ratings for two items. In place of the item-profile vector for an item, we use its column in the utility matrix. User are similar if their vectors are close according to some notion of distance measures such as cosine distance. The process of identifying similar user and recommending what similar users like is called collaborative filtering.
\chapter{Creation of a Content-based Recommender system}\label{ch:recsyscontbased}
Before I talk about the content-based recommendation system that I created from scratch, I want to highlight some of the steps that allowed me to arrive at the solution or at least allowed me to learn more about the data I had at my disposal.
\section{Business similarity suggestion with Pandas and cosine similarity}\label{bussimpand}
First of all I tried to create a suggestion system by looking the business similarity. For achieve this goal I used in this step Pandas. I performed this steps:
\begin{enumerate}
\item Extract the reference restaurant index
\item Mapping the rate (1-3 stars will be mapped in 0 and 4-5 in 1)
\item Create the \texttt{cosine\textunderscore similarity\textunderscore scratch} function
\begin{lstlisting}[language={Python},escapechar=|,label={lst:cosinesim},caption={Cosine similarity function}]
def cosine_similarity_scratch(vector1, vector2):
    dot_product = np.dot(vector1, vector2)
    norm_vector1 = np.linalg.norm(vector1)
    norm_vector2 = np.linalg.norm(vector2)
    
    if norm_vector1 == 0 or norm_vector2 == 0:
        return 0.0
    
    similarity = dot_product / (norm_vector1 * norm_vector2)
    return similarity

\end{lstlisting}
\item Calculate business similarity respect the one selected. Order the similar restaurants in a descending way and then select n (= 5 in this case) top similar restaurant and print it.
\begin{lstlisting}[language={Python},escapechar=|,label={lst:simpandas},caption={Suggestion system in Pandas}]
restaurant_features = features_df.iloc[restaurant_index, 6:-2].values  # Reference restaurant feature vector

similarities = []
for i in range(len(features_df)):
    if i != restaurant_index:
        other_restaurant_features = features_df.iloc[i, 6:-2].values
        similarity = cosine_similarity_scratch(restaurant_features, other_restaurant_features)
        similarities.append(similarity)

similarities = np.array(similarities)
similar_restaurants = similarities.argsort()[::-1]  # Similar restaurant ordered in a desc way

# Visualize 5 top suggested restaurants with cosine sim column
top_similar_restaurants = similar_restaurants[:5]
recommended_restaurants = df_total.iloc[top_similar_restaurants][['name', 'stars']]
recommended_restaurants['cosine_similarity'] = similarities[top_similar_restaurants]

print(recommended_restaurants)

\end{lstlisting}
\end{enumerate}
So now I will show the results obtained.
\subsection{Experiment}
I choose the \texttt{8071} restaurant index called \texttt{Adelita Taqueria \& Restaurant}.
It's a mexican restaurant located in 1108 S 9th St Philadelphia, PA 19147. In the note the link of the restaurant on Yelp.\footnote{\url{https://www.yelp.com/biz/adelita-taqueria-and-restaurant-philadelphia?osq=Adelita+Taqueria\%26+Restaurant}}
The restaurant counts 39 reviews with an average rating of 4.5 stars.\par
The similar business that the system recommend are these reported in the Table \ref{tab:simrespand} where there are linked their Yelp page.
\begin{table}[]
\caption{Top similar business recommended in Pandas}
\label{tab:simrespand}
\begin{tabular}{|l|l|l|l|}
\hline
     & name                           & stars & cosine\_similarity \\ \hline
7109 & \href{https://www.yelp.com/biz/los-taquitos-de-puebla-philadelphia-4?osq=Los+Taquitos+de+Puebla}{Los Taquitos de Puebla}         & 4     & 1.000000           \\ \hline
5990 & \href{https://www.yelp.com/biz/san-antonio-mexican-cousine-hatfield?osq=San+Antonio+Mexican+cousine}{San Antonio Mexican cousine}    & 5     & 1.000000           \\ \hline
3534 & \href{https://www.yelp.com/biz/los-cuatro-soles-philadelphia?osq=Los+Cuatro+Soles}{Los Cuatro Soles}              & 5     & 0.942809           \\ \hline
3292 & \href{https://www.yelp.com/biz/ecowas-african-restaurant-philadelphia-2}{El Limon - Norristown}          & 4     & 0.942809           \\ \hline
1525 & \href{https://www.yelp.com/biz/la-hacienda-mexican-restaurant-bensalem?osq=La+Hacienda+Mexican+Restaurant}{La Hacienda Mexican Restaurant} & 4     & 0.942809           \\ \hline
\end{tabular}
\end{table}
They are all mexican restaurant so it's perfectly matching our selected restaurant.
I tried also to use a cosine similarity library from \texttt{from sklearn.metrics.pairwise import cosine\_similarity} and the results are quite similar as shown in Table \ref{tab:simrespandsklearn}.
\begin{table}[]
\caption{Top similar business recommended in Pandas with sklearn cosine similarity}
\label{tab:simrespandsklearn}
\begin{tabular}{|l|l|l|}
\hline
name                        & cosine similarity  & stars \\ \hline
San Antonio Mexican cousine & 0.9999999999999999 & 5     \\ \hline
Los Taquitos de Puebla      & 0.9486832980505138 & 4     \\ \hline
\href{https://www.yelp.com/biz/ecowas-african-restaurant-philadelphia-2}{El Charro Negro            } & 0.9486832980505138 & 4     \\ \hline
El Limon - Norristown       & 0.9128709291752769 & 4     \\ \hline
\href{https://www.yelp.com/biz/plaza-garibaldi-philadelphia-3?osq=Plaza+Garibaldi}{Plaza Garibaldi}             & 0.8999999999999999 & 4     \\ \hline
\end{tabular}
\end{table}
\section{Business similarity suggestion with k-NN algorithm}\label{sec:knn-sim}
I continued my search for similar restaurants also based on the k-nn algorithm inspired by a kaggle notebook \cite{notekaggle}. In brief it's a a supervised machine learning approach called k-nearest neighbors (k-NN) and it is utilized for both classification and regression problems. It is a non-parametric algorithm that bases its predictions on how closely new input data and training examples resemble each other. \par
The "k" in the k-NN method denotes how many nearest neighbors are taken into account while producing predictions.
The steps done are the following: 
\begin{itemize}
\item Take features excluding name and stars from the \texttt{df\_total} as x and stars as y
\item Splitting the data into training set and test set 
\item To figure out which k to choose, I applied 5-fold cross-validation and the best value for k is 20.
\item Then I fitted the model and printed the results \par
\texttt{Score on training set: 0.5624903205823137 ; Score on test set: 0.52198142486068}
\item Testing the model and use the last row as the validation test 
\item Getting the similar restaurants compared with the one taken in the validation test.
\end{itemize}
In the end, the results obtained are comparable with the other two approaches (cosine similarity from scratch and sklearn).
\section{Business similarity suggestion with PySpark and cosine similarity}
In this section I wanted to focus more on the scalability of the system. Therefore, I converted the approach used in section \ref{bussimpand} to PySpark, performing all the preprocessing by obtaining \texttt{df\_final\_spark}. Specifically, I summarize the operations:
\begin{itemize}
\item Transform the categories into a list of categories for the business
\item Perform filtering by state and business type.
\item Extract the attributes as columns and then having some columns of nested attributes I re-run the operation
\item Transform the values to 0-1 obtaining the \texttt{df\_business\_attr\_spark}
\item Same for categories creating the \texttt{df\_categories\_dm}
\item Finally I get the \texttt{df\_final\_spark} by the join between the categories,attributes and the \texttt{restaurant\_df\_sel\_spark} which contains the name and ratings of the various businesses.
\end{itemize}
Once I had completed all the preprocessing phase and obtained the various df's I could proceed with the recommendation. As seen in the code shown I created the \texttt{df\_matrix} that contains \textit{business\_id,features}. The features were generated using \texttt{VectorAssembler}.It is a feature transformer that combines multiple input columns into a single vector column.
\begin{lstlisting}[language={Python},escapechar=|,label={lst:simpyspark},caption={Suggestion system in PySpark}]
# Exclude the 'business_id' column from df_final
columns = [c for c in df_final_spark.columns if c != 'business_id']

# Convert binary values to integers
df_numeric = df_final_spark.select(['business_id'] + [col(c).cast('integer') for c in columns])

# Fill NaN values with 0
df_filled = df_numeric.fillna(0, subset=columns)

# Use VectorAssembler to create a feature vector column
assembler = VectorAssembler(inputCols=columns, outputCol='features')
df_matrix = assembler.transform(df_filled).select('business_id', 'features')
\end{lstlisting}
After that I proceeded to run the same test on the \textit{Adelita Taqueria \& Restaurant} using \texttt{cosine\_similarity\_spark} slightly modified to make it work properly with the PySpark pipeline.
In this case I wanted to select 15 as the number of recommendations.This is the Table \ref{tab:topbuspyspark} obtained:\par
\begin{table}[h]
\caption{Top similar business recommended in PySpark}
\label{tab:topbuspyspark}
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{business\_id}            & \multicolumn{1}{c|}{cosine\_similarity} & \multicolumn{1}{c|}{name}                        & \multicolumn{1}{c|}{stars} \\ \hline
\multicolumn{1}{|c|}{F2QwLwzS3vF9os0...} & \multicolumn{1}{c|}{{[}{[}1.0{]}{]}}    & \multicolumn{1}{c|}{San Antonio Mexican cousine} & \multicolumn{1}{c|}{4.5}   \\ \hline
DwOhLOd9Say...   & {[}{[}0.9803789354850793{]}{]} & Los Taquitos de Puebla & 3.5 \\ \hline
EoQiJ5D-pyWc...  & {[}{[}0.9714285714285714{]}{]} & El Primo Taqueria      & 4.5 \\ \hline
XQewVfTaosZ3U-4g...  & {[}{[}0.9714285714285714{]}{]} & Los Cuatro Soles       & 4.5 \\ \hline
mKJ\_WV7TvrjyDjm... & {[}{[}0.9710083124552245{]}{]} & El Limon - Bensalem    & 4.5 \\ \hline
SVf23pjKERkedqCdWl6ECA   & {[}{[}0.9660917830792959{]}{]} & Teresa's Mesa          & 4.0 \\ \hline
5ItgryJvadUrKVljjJ8l4g   & {[}{[}0.9660917830792959{]}{]} & Que Chula Es Puebla    & 4.0 \\ \hline
rNg75hKR0UIB5-jX5WiVQg   & {[}{[}0.9660917830792959{]}{]} & Los Mariachis          & 3.5 \\ \hline
1MVMKUvZfWwIqkhxP3rYvQ   & {[}{[}0.9613406389911041{]}{]} & Indian Garden          & 4.0 \\ \hline
sKUs4ISUgn3j6SeLYvSByg   & {[}{[}0.9613406389911041{]}{]} & El Charro Negro        & 4.0 \\ \hline
x1CtK2qnlCr\_1DJFNo1\_vw & {[}{[}0.9583148474999098{]}{]} & El Limon - Norristown  & 4.0 \\ \hline
venWfi69QVylbyrvaPt0nQ   & {[}{[}0.9578414886923188{]}{]} & Cafe Ynez              & 4.5 \\ \hline
t0Qyogb4x--K9i5b0AoDCg   & {[}{[}0.9578414886923188{]}{]} & Taco Maya              & 4.5 \\ \hline
fcAaBcYFo1YqetIjs9l8Iw   & {[}{[}0.9566222088265397{]}{]} & Sungate Diner          & 4.5 \\ \hline
uh-387pbyipzjCMcj5Dy6w   & {[}{[}0.9566222088265397{]}{]} & Las Palmas Del Sur     & 4.5 \\ \hline
\end{tabular}
\end{table}
Also you can see that they are all restaurants that cook Mexican style. But these methods are based solely and exclusively on the profile of the restaurants without taking into account the feedback that the user has given to the various restaurants. For this reason, I decided to continue the project by introducing the user profile component as well.
\section{Collaborative filtering with ALS ml library}
In collaborative filtering, matrix factorization is the state-of-the-art solution for sparse data problem, although it has become widely known since Netflix Prize Challenge.
A matrix factorization is a factorization of a matrix into a product of matrices. 
In the case of collaborative filtering, matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices.One matrix can be seen as the user matrix where rows represent users and columns are latent factors. The other matrix is the item matrix where rows are latent factors and columns represent items \cite{matrixfact}. Here's a picture that describe the matrix factorization.
\begin{figure}[hbtp]
\caption{Example of Matrix Factorization}
\centering
\includegraphics[scale=0.6]{../Images/matrixfact.png}
\end{figure}
The advantages of using this approach are mainly two:
\begin{itemize}
\item Model learns to factorize rating matrix into user and business representations, which allows model to predict better personalized business ratings for users
\item Less-known business can have rich latent representations as much as popular business have
\end{itemize}
More formally, in a sparse user-item interaction matrix the predicted rating user \textit{u} will give item \textit{i} is computed as:
$$r_{ui} = \sum_{f=0}^{nfactors} H_{u,f}W_{f,i}$$
Rating of item i given by user u can be expressed as a dot product of the user latent vector and the item latent vector.  Latent factors are the features in the lower dimension latent space projected from user-item interaction matrix.\par
Increasing the number of latent factors will improve personalization, until the number of factors becomes too high, at which point the model starts to overfit. A common strategy to avoid overfitting is to add regularization terms to the objective function.\par
The objective of matrix factorization is to minimize the error between true rating and predicted rating.
\subsection{Alternating Least Square (ALS) with Spark ML}\label{sec:als}
Alternating Least Squares (ALS) is a parallel matrix factorization algorithm. It is implemented in Apache Spark ML and specifically designed for large-scale collaborative filtering problems. ALS effectively addresses scalability and sparsity issues commonly encountered with Ratings data. Furthermore, ALS exhibits simplicity and demonstrates excellent scalability even when handling massive datasets. The main points are:
\begin{itemize}
\item ALS uses L2 regularization
\item ALS minimizes two loss functions alternatively; It first holds user matrix fixed and runs gradient descent with item matrix; then it holds item matrix fixed and runs gradient descent with user matrix
\item Its scalability: ALS runs its gradient descent in parallel across multiple partitions
\end{itemize}
So let's get to the heart of the matter by taking a look at the code.
Starting with \texttt{df\_review\_spark\_indexed} I go to convert the profile ids to numeric format using the \texttt{StringIndexer}. Once this is done we go to split the data into training set and test set as reported by this code:
\begin{lstlisting}[language={Python},escapechar=|,label={lst:splittraintest},caption={Splitting dataset in training and test set for ALS}]
(training, test) = df_review_spark_indexed.randomSplit([0.8, 0.2])
\end{lstlisting}
After that, the recommendation model based on the ALS model is created:
\begin{lstlisting}[language={Python},escapechar=|,label={lst:splittraintest},caption={Building recommended system model based on the ALS model}]
# Build the recommendation model using ALS on the training data
als = ALS(maxIter=5, regParam=0.01, userCol="user_id_index", itemCol="business_id_index", ratingCol="stars_review")
model = als.fit(training)
\end{lstlisting}
There are some parameters that need to be considered such as:
\begin{itemize}
\item \texttt{maxIter}: is the maximum number of iterations to run
\item \texttt{regParam}: specifies the regularization parameter in ALS
\item \texttt{userCol}: as suggested from the name is the column of the user 
\item \texttt{itemCol}: as suggested from the name is the column of the item 
\item \texttt{ratingCol}: as suggested from the name is the column of the rating 
\end{itemize}
This operation requires times according to the type of technical specifications you have. Those used in the project are specified in the section \ref{sec:sysspec}.
Once we have fitted the model with the training data we can proceed to test our model with the test set.
\begin{lstlisting}[language={Python},escapechar=|,label={lst:predals},caption={Prediction over the test set with the ALS model}]
predictions = model.transform(test)
#predictions.show()
\end{lstlisting}
Then I proceeded to test the model by filtering for a generic user id which in this case is \texttt{user\_id\index = 14269.0} by descendingly sorting the prediction column. \par
To then understand the goodness-of-fit of the model, I calculated the NDCG (see the section \ref{sec:ndcg}) by manipulating the df columns as can be seen from the code:
\begin{lstlisting}[language={Python},escapechar=|,label={lst:ndcgals},caption={Calculating the ndcg over the predictions df obatined in ALS}]
from pyspark.sql.functions import expr

predictions = predictions.withColumn("rank", expr("row_number() over (partition by user_id_index order by prediction desc)"))
predictions = predictions.withColumn("dcg", expr("1 / (log2(rank + 1))"))
predictions = predictions.withColumn("idcg", expr("1 / (log2(1 + rank))"))
ndcg = (
    predictions
    .groupBy("user_id_index")
    .agg(expr("sum(dcg) as dcg_sum"), expr("sum(idcg) as idcg_sum"))
    .select(expr("avg(dcg_sum / idcg_sum)").alias("ndcg"))
    .collect()[0][0]
)
\end{lstlisting}
Printing the metric we get \texttt{ndcg = 1.0}. A value of NDCG of 1 indicates that the system has produced perfect recommendations corresponding to user preferences.
\section{Content based recommendation from scratch in PySpark}\label{sec:cbrfsip}
Finally we enter in the main section about the project. We introduce how I created a \textit{Content-based} reccomendation system from scratch in PySpark.
To do this, I started by constructing the \texttt{df\_final\_content\_spark} which contains all the information related to the restaurants such as the categories and attributes coded in 0-1, its rating (\textit{stars}) and its \textit{business\_id}. Through the latter it is possible to make a join with \texttt{df\_review\_spark\_indexed} so that we also get the information about the user and his explicit rating for a given business through the review.\par
I decided in order to speed up the computation to reduce the data according to this operation:
\begin{lstlisting}[language={Python},escapechar=|,label={lst:fractiondata},caption={Fractioning data}]
merged_df = merged_df.sample(fraction=0.001, seed=42)
#merged_df.count()
\end{lstlisting}
Nevertheless we have a good amount to proceed the with the recommendation system.\par
To make the system user-centric, I weighted the matrix obtained with the user's ratings in such a way that it is considered in the system. In fact here is the code:
\begin{lstlisting}[language={Python},escapechar=|,label={lst:weightedmatrix},caption={Weighted the business matrix with the user review rate}]
# Select all columns and excluding these 'business_id_index', 'stars_review', 'user_id_index'
columns_to_multiply = [column for column in merged_df.columns if column not in ['business_id_index', 'stars_review', 'user_id_index','stars']]

# Perform the product over each column
for column in columns_to_multiply:
    weighted_merged_df = merged_df.withColumn(column, col(column) * col('stars_review'))
# this is the weighted cat matrix
#weighted_merged_df.show()
\end{lstlisting}
This is useful to me because by grouping by user\_id and summing over the columns I get \texttt{user\_profile}.
However I need to do some preprocessing until I get the normalized user profile called in the code \texttt{user\_profile\_normalized\_df}.\par
So the in the same matter we can obtain \texttt{business\_profile\_normalized\_df}.
Finally with this two profiles we can create the matrix with a crossJoin like in snippet:
\begin{lstlisting}[language={Python},escapechar=|,label={lst:crossbus},caption={Cross join between profiles}]
# Union of business profiles and user profiles
cross_business_user = business_profile_normalized_df.crossJoin(user_profile_normalized_df)
\end{lstlisting}
The structure of \texttt{cross\_business\_user}  is shown in the Table \ref{tab:crossbustab}
\begin{table}[]
\caption{Cross\_business\_user show example}
\label{tab:crossbustab}
\begin{tabular}{|l|l|l|l|}
\hline
business\_id\_index & user\_id\_index & user\_features        & business\_features     \\ \hline
128.0               & 14269.0         & (289,{[}3,9,22,23,2.. & (292,{[}0,1,5,11,24... \\ \hline
\end{tabular}
\end{table}
As you can see the features have different dimensions. So before calculating the cosine similarity we need to fix this issue. To aim this here the snippet:
\begin{lstlisting}[language={Python},escapechar=|,label={lst:featuresalign},caption={Cross join between profiles}]
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

# Cosine similarity function
def cosine_similarity(v1, v2):
    dot_product = float(v1.dot(v2))
    norm_v1 = float(v1.norm(2))
    norm_v2 = float(v2.norm(2))
    similarity = dot_product / (norm_v1 * norm_v2)
    return similarity

# UDF to truncate or expand feature vectors
def resize_features(features, max_dimension):
    return Vectors.sparse(max_dimension, features.indices, features.values)

# UDF to calculate cosine similarity as a column in a DataFrame
cosine_similarity_udf = udf(cosine_similarity)


# Find the maximum size between the two DataFrames
max_dimension = max(cross_business_user.withColumn("size", udf(lambda x: x.size, IntegerType())("business_features")).selectExpr("max(size)").collect()[0][0],
                    cross_business_user.withColumn("size", udf(lambda x: x.size, IntegerType())("user_features")).selectExpr("max(size)").collect()[0][0])

# Truncation or expansion of feature vectors
cross_business_user = cross_business_user.withColumn("business_features_resized", udf(lambda x: resize_features(x, max_dimension), VectorUDT())("business_features")) \
                                         .withColumn("user_features_resized", udf(lambda x: resize_features(x, max_dimension), VectorUDT())("user_features"))
\end{lstlisting}
As you can see we find the maximum dimension and we check if the column should be expanded or restricted based on the max.
Now it's possible to apply \texttt{cosine\_similarity} between the business features and the user features get \texttt{similarity\_df}. Look at the schema in Listing \ref{lst:schemasim}:
\begin{lstlisting}[language={bash},label={lst:schemasim},caption={Similarity df schema}]
root
 |-- business_id_index: double (nullable = false)
 |-- business_features: vector (nullable = true)
 |-- user_id_index: double (nullable = false)
 |-- user_features: vector (nullable = true)
 |-- business_features_resized: vector (nullable = true)
 |-- user_features_resized: vector (nullable = true)
 |-- similarity: string (nullable = true)
\end{lstlisting}
There is the column \texttt{similarity} that allows us to perform the recommendation.
Lets select our user of interest, in this case, the same one that we have choose in the section \ref{sec:als}. Then we order the similarity in desc way and filter by the user of interest.\par
Now is time to show the recommended business:
\begin{lstlisting}[language={Python},label={lst:recommendedbusinesses},caption={Recommended businesses calculation}]
#Union with the DataFrame business_profiles_df to obtain information about recommended businesses
df_res_sel = df_res_sel.withColumnRenamed('business_id_index','df_res_sel_business_id_index')
recommended_businesses = user_similarity.join(df_res_sel, user_similarity.business_id_index == df_res_sel.df_res_sel_business_id_index, how='inner').drop('df_res_sel_business_id_index').select("business_id_index","name","business_features","user_id_index","user_features","business_features_resized","user_features_resized","similarity")

#Visualize the recommended business
#recommended_businesses.show(5)
\end{lstlisting}
To understand if the recommendation are good enough we need to interpreter the data. Indeed 
after that, I created a \texttt{historical\_data\_stars} that is useful to our prediction.
Indeed \texttt{recommended\_businesses} is a join of dfs that give us a complete view of the prediction.\par
We build the prediction df in this way: 
\begin{lstlisting}[language={Python},label={lst:predictiondf},caption={Build prediction df}]
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, DoubleType

# Define a UDF (User Defined Function) to convert the struct column to an array of double
convert_array_udf = udf(lambda struct_col: struct_col["values"], ArrayType(DoubleType()))

# Apply the UDF to convert the column to the correct type in 'recommended_businesses'
recommended_businesses = recommended_businesses.withColumn("business_features", convert_array_udf("business_features"))

ground_truth = cross_business_user.withColumn("business_features_resized", convert_array_udf("business_features_resized"))


# Perform the join and select the desired columns
predictions = recommended_businesses.join(historical_data_stars, ["user_id_index", "business_id_index"], how="inner") \
                                    .na.fill(0) \
                                    .select("user_id_index", "business_id_index", "business_features", "stars_review")
\end{lstlisting}
so by joining the historical data with the recommended\_business. \par
I finally create a single joined df that contains all the necessary information on which I can perform the normalized discounted cumulative gain calculation.
The result is \texttt{NDCG: 0.33141829507855797}.
\chapter{Scalability and complexity}\label{ch:scalability}
So let us discuss the scalability and complexity aspects. Starting from the K-Nearest Neighbors algorithm (section \ref{sec:knn-sim}) used in the code of the project in, the complexity will depend mainly on the size of the data set and the value of K. The computational complexity of the K-NN is O(n * m * d), where:
\begin{itemize}
\item n is the number of points in the data set (user or business profiles)
\item m is the number of neighbors to be considered (value of K)
\item d is the size of the feature vector (number of attributes or profile dimensions)
\end{itemize}
Thus, if you have a large dataset and a high value of K, the complexity can become significant. However, K-NN can be parallelized and scaled easily on distributed architectures such as Apache Spark, allowing large datasets to be handled.\par 
If, on the other hand, we talk about ALS, its training complexity is dominated by the number of iterations and the number of latent (rank) factors specified. Specifically, the complexity for each iteration of the ALS algorithm is approximately O(m * n * rank), where m is the number of users and n is the number of recommendable items. \par 
On the other hand, as for the content-based recommender system from scratch in PySpark we have to consider that the creation of the weighted matrix for example is O(n) where n is the number of features. If we consider the code provided for the calculation of the cross\_bussiness\_user matrix and all the above operations in the Listing \ref{lst:featuresalign} then we can say:
\begin{itemize}
\item Calculation of cosine similarity (cosine\_similarity): the complexity will depend mainly on the size of the feature vectors passed as input (v1 and v2). Assuming they have size n then it will be O(n)
\item UDF for resizing feature vectors (resize\_features): assuming the number of nonzero elements is m then it will be O(m)
\item Calculating the maximum size between the two DataFrames: assuming they have k rows the complexity will be O(k).
\item Resizing feature vectors in DataFrames: . If we assume that the number of rows is k and the number of nonzero elements in the feature vectors is m, then the complexity will be O(k * m).
\end{itemize}
So in general we can approximate the computations of this important stage of the recommender system as an O(n + m + k + k * m).
In general using an approach such as PySpark allows us to exploit some aspects related to distributed computation by dividing the work across several machines and solving problems in parallel. Also if we had an HDFS like Apache Hadoop available we could use partitions in such a way as to balance the load.
\section{System specification}\label{sec:sysspec}
All the code was executed on \textit{Google Colab}. Date of last execution: \texttt{06/06/2023}. The System specification are reported here:
\begin{itemize}
\item Disk information
\begin{lstlisting}[language={bash},label={lst:diskinfo},caption={Disk information}]
!df -h

Filesystem      Size  Used Avail Use% Mounted on
overlay         108G   24G   85G  22% /
tmpfs            64M     0   64M   0% /dev
shm             5.8G     0  5.8G   0% /dev/shm
/dev/root       2.0G 1005M  952M  52% /usr/sbin/docker-init
tmpfs           6.4G  220K  6.4G   1% /var/colab
/dev/sda1        41G   25G   16G  61% /etc/hosts
tmpfs           6.4G     0  6.4G   0% /proc/acpi
tmpfs           6.4G     0  6.4G   0% /proc/scsi
tmpfs           6.4G     0  6.4G   0% /sys/firmware
\end{lstlisting}
\item CPU specs
\begin{lstlisting}[language={bash},label={lst:cpuspec},caption={CPU spec}]
!cat /proc/cpuinfo

processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 79
model name	: Intel(R) Xeon(R) CPU @ 2.20GHz
stepping	: 0
microcode	: 0xffffffff
cpu MHz		: 2199.998
cache size	: 56320 KB
physical id	: 0
siblings	: 2
core id		: 0
cpu cores	: 1
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed
bogomips	: 4399.99
clflush size	: 64
cache_alignment	: 64
address sizes	: 46 bits physical, 48 bits virtual
power management:

processor	: 1
vendor_id	: GenuineIntel
cpu family	: 6
model		: 79
model name	: Intel(R) Xeon(R) CPU @ 2.20GHz
stepping	: 0
microcode	: 0xffffffff
cpu MHz		: 2199.998
cache size	: 56320 KB
physical id	: 0
siblings	: 2
core id		: 0
cpu cores	: 1
apicid		: 1
initial apicid	: 1
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities
bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed
bogomips	: 4399.99
clflush size	: 64
cache_alignment	: 64
address sizes	: 46 bits physical, 48 bits virtual
power management:
\end{lstlisting}
\item Memory info
\begin{lstlisting}[language={bash},label={lst:meminfo},caption={Memory information}]
!cat /proc/meminfo

MemTotal:       13294264 kB
MemFree:         9219976 kB
MemAvailable:   12020892 kB
Buffers:           62876 kB
Cached:          2930080 kB
SwapCached:            0 kB
Active:           854108 kB
Inactive:        2969768 kB
Active(anon):       1140 kB
Inactive(anon):   831364 kB
Active(file):     852968 kB
Inactive(file):  2138404 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:               132 kB
Writeback:             0 kB
AnonPages:        829632 kB
Mapped:           280996 kB
Shmem:              1584 kB
KReclaimable:     100996 kB
Slab:             137044 kB
SReclaimable:     100996 kB
SUnreclaim:        36048 kB
KernelStack:        4492 kB
PageTables:        18972 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     6647132 kB
Committed_AS:    1987656 kB
VmallocTotal:   34359738367 kB
VmallocUsed:        9444 kB
VmallocChunk:          0 kB
Percpu:             1320 kB
HardwareCorrupted:     0 kB
AnonHugePages:     22528 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
CmaTotal:              0 kB
CmaFree:               0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB
DirectMap4k:       82744 kB
DirectMap2M:     4108288 kB
DirectMap1G:    11534336 kB
\end{lstlisting}

\end{itemize}
\chapter{Results and conclusion}\label{ch:resultsconc}
To evaluate the system I used the Normalized Discounted Cumulative Gain (NDCG).
\section{Normalized Discounted Cumulative Gain (NDCG)}\label{sec:ndcg}
Before talking about the NDCG we need to understand what is CG (Cumulative Gain) and DCG (Discounted Cumulative Gain) \cite{ndcgtow}. Then there are two main assumptions:
\begin{itemize}
\item  Highly relevant documents are more useful when appearing earlier in the search engine results list.
\item Highly relevant documents are more useful than marginally relevant documents, which are more useful than non-relevant documents
\end{itemize}
\subsubsection{CG}
If every recommendation has a graded relevance score associated with it, CG is the sum of graded relevance values of all results in a search result list as reported in this formula:
$$CG_{p} = \sum_{i=1}^{p}rel_{i}$$ 
The Cumulative Gain at a particular rank position p, where the $rel_{i}$ is the graded relevance of the result at position i.\par
The problem with CG is that it does not take into consideration the rank of the result set when determining the usefulness of a result set.
\subsubsection{DCG}
To overcome this we introduce DCG. DCG penalizes highly relevant documents that appear lower in the search by reducing the graded relevance value logarithimically proportional to the position of the result.
$$DCG_{p} = \sum_{i=1}^{p}\frac{2^{rel_{i}} - 1}{log_{2}(i+1)}$$
\subsubsection{NDCG}
An issue arises with DCG when we want to compare the search engines performance from one query to the next because search results list can vary in length depending on the query that has been provided.We perform this by sorting all the relevant documents in the corpus by their relative relevance producing the max possible DCG through position p (a.k.a Ideal Discounted Cumulative Gain). Here the formula:
$$nDCG_{p} = \frac{DCG}{IDCG_{p}}$$ where 
$$IDCG_{p} = \sum_{i=1}^{|REL_{p}|} \frac{2^{rel_{i}} - 1}{log_{2}(i+1)}$$
REL\_p represents the list of relevant documents (ordered by their relevance) in the corpus up to position p.\par
The ratios will always be in the range of [0, 1] with 1 being a perfect score — meaning that the DCG is the same as the IDCG.  It was what happened in the section \ref{sec:als}. With the recommender system seen in the section \ref{sec:cbrfsip} the \texttt{NDCG: 0.33141829507855797}.This would indicate a low result suggesting recommended results that may not be very relevant. It could have been influenced by the reduction of the dataset or perhaps more user-related information could also be included through a textual analysis on the reviews.



\lstlistoflistings
\listoffigures
\listoftables
\bibliographystyle{unsrt}
\bibliography{biblio}
\end{document}
